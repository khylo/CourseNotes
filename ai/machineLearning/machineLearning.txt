Machine Learning Algorithems
	~Supervised Learning
	- Unsupervised learning
	others... Reinforccement learning/ recommender systems

Octave
	tips 
	http://www.mathworks.com/help/matlab/matlab_prog/vectorization.html
	
	
	

Supervised Learning.
	2 sub categories. 
		Regression, where we try to predict a continuos value for future data based on past data (Graph fitting etc), eg. house prices etc.
			Notation
				m = number of training examples
				x's = inpout features/ variables
				y's output variable
				x,y = single training example
				h = hypothesis. i.e. the predicting algorithm... h maps from x to y.. 
				ho(x) = O(0) + O(1)X ...  O = thetha = paramters of the model. O(0) is constant. O(1) is multiplier of x axis
			Linear regresseion.. with one variable/ univariate)
				Cost function J = Min over O(0), O(1) 1/2m Sum (h(x ) - y) squared
			Gradient Descent
				Oj := Oj - a J(Oo,O1) for j=0 and j=1
				a= alpha = learning rate.. i.e. how big the steps,    := asignemnt as opposed to = which is logical op
				Vectorised :
					J(thetha)=1/2m * (X.theta-Y)' .(X.theta-Y)
				Grad descent 
					Theta = Theta- (alpha/m . X'(X.Theta-Y)) Note Theta = vector of thetas
				
			Normal Equation
				O = pinv(XtX) *Xt    Note pinv= pseudo inverse.. Calculates even if X'X is not invertible.. (inv is pure inverse)
				in Octave theta = pinv(X*X')*X'*y
				Non invertible if
					If 2 features are related (redundant) e.g. x1 size in feet, x2 size in meters.. Then X can be no invertable.
					too many features.. m (training set) <= n (feautres)
				
				
				slow for large n.. Gradient descent works better for large n (number of features)
				Computing pinv(X) is O(n^3) so very slow... start to consider using GD when n> 10000
			
		Classification. Predict a discrete value depending on various input factors. Eg. was account hacked or not.
			Can graph based on criteria
			|    x x
			| o x 
			| x   o
			-----------
			
			Logistic regression or sigmoid function
			h(theta) = g(theta' X)
				g(z) = 1/(1+e^-z)
				
				
			Cost function  (note h(theta) is the hypotesis for x...  = theta'X.. we are trying to find minimum theta for training set X and Y)
			J(theta)  = (1/ m). Sum(Cost)
				Cost (h(x), y) = .. y==1 -> -log( h(x))               if y==0 -> -log(1-h(x))
			or in summary
				Cost = -y.log(h(x)) - (1-y).log(1-h(x)) .. Note this is same as above since we factor in y value. Y always = 0 or 1 for classification
			

REgularization
		Underfitting vs overfitting (too many features or too complex curve)
		We can add in multiplication factor to the cost function to reduce the effect of higher theta's 
		e.g. J = J +1000 theta(4)+1000 theta(5)
		
		or use regularization   lambda * theta / m       note not for theta(0)
		
		Strangely regularization works well for normal eqn for linear regression too.
		
		X'X + lambda (zeroEye(length(y)+1)) is always invertible.. whereas without the lambda its not necessarily if n<m
			
		
Unsupervised learning
	Clustering algorithm.. Can you find patterns .. e.g. organise computing clusters/ social network analysis (firends recomendation)/ Market segmentation.. place users into market segments/ Astonomical data analysis
	
	Coctail party problem
		[W,s,v]  svd((repmat(sum(x.*x,1), size(x,1), 1).*x)*x');
			svd = single value decomposition
		
	Clustering
	K-means clustering algorithm (K is the number of clusters.. and m X's inputs).. x is n-dimensional vector
		Create 2 points.. cluster centroids.. (we are grouping into 2 clusters)
			- randomly initialize K cluster centroids u1 - uk
				for each X in 1 to m
					c(i) = index (1 to K) of cluster centroid closest to x  => | xi - uk| ^2 for each u until we find smallest value
				for k = 1 to k
					uk = mean of points assigned to cluster k
					
			if no points assigned can either 1/ Drop that cluster.. or 2. re-randomly assign k centroids.. (perhaps the original placings were bad)
		Multiple random starts if k small (<10)
		
			elbow method somethimes used to choose # k.. (# clusters) 
				plot Cost fn vs number of k.. (can get obtuse angle.. elbow point is considered 'good' choice of k)
					sometimes don't get elbow graph.. no obvious elbow
					
			also can plot data and see how many clusters make sence (e.g. t-shirt size)
			
			
Dimensionality Reduction (helps reduce feautes.. and increases speed)
	e.g. reduce dependent features. (e.g. measurement in inches/ measurement in cm... or ... peoples attiude/ peoples opinion )
	
	Graphically project.. e.g. 2 features will map to strasightish line. These can be projected down to new line and those can be mapped
		(note not necessarily taking one of the other)
		or can take sums or products./ /e.g. countrySize/ GDP
		
	PCA Principal Component Analysis.. Project high dimension data onto lower dimension (<=3)
	
BigData
	Batch Gradient descent (as done previously)
	Stochastic gradient descent (one at a time).. rotates around min (but starts giving results after 1 iteration not like batch)
	Mini-Batch GD
		Take a small batch of say 10 - 100
			Only better than stochastic if you have a good vectorization implementation (which will parallelise for you)
	
	
Photo OCR
	www.pdsounds.org =  site with opensource audio examples
	
	

Examples using Mahout
	
	
Mahout
	
1/ Exampels from Hadoop in Practise chapter 9 (http://techbus.safaribooksonline.com/9781617290237/kindle_split_020_html#X2ludGVybmFsX0h0bWxWaWV3P3htbGlkPTk3ODE2MTcyOTAyMzclMkZjaDA5bGV2MXNlYzFfaHRtbCZxdWVyeT0oKG1haG91dCkp)
	Run recommender system using data from  http://www.grouplens.org/node/12 (movie recommendations)
		Ratings are given in format UserID::MovieID::Rating::Timestamp
		User Details are given in UserID::Gender::Age::Occupation::Zip-code
		Convert ratings to csv using ..  (awk -F "::" '{print $1","$2","$3}' ratings.dat > ratings.csv)
		hadoop fs -put ratings.csv
		Add user-ids.txt file..  fields (e.g. using first 3 userId's )
	
	
	mahout     \
        recommenditembased \
         -Dmapred.reduce.tasks=10 \
        --similarityClassname SIMILARITY_PEARSON_CORRELATION \
        --input ratings.csv \
        --output item-rec-output \
        --tempDir item-rec-tmp \
        --usersFile user-ids.txt
		
	Output
		13/01/27 17:55:21 INFO mapred.JobClient: Job complete: job_201301070858_0158
		13/01/27 17:55:21 INFO mapred.JobClient: Counters: 33
		13/01/27 17:55:21 INFO mapred.JobClient:   File System Counters
		13/01/27 17:55:21 INFO mapred.JobClient:     FILE: Number of bytes read=41274
		13/01/27 17:55:21 INFO mapred.JobClient:     FILE: Number of bytes written=3155647
		13/01/27 17:55:21 INFO mapred.JobClient:     FILE: Number of read operations=0
		13/01/27 17:55:21 INFO mapred.JobClient:     FILE: Number of large read operations=0
		13/01/27 17:55:21 INFO mapred.JobClient:     FILE: Number of write operations=0
		13/01/27 17:55:21 INFO mapred.JobClient:     HDFS: Number of bytes read=2671506
		13/01/27 17:55:21 INFO mapred.JobClient:     HDFS: Number of bytes written=271
		13/01/27 17:55:21 INFO mapred.JobClient:     HDFS: Number of read operations=240
		13/01/27 17:55:21 INFO mapred.JobClient:     HDFS: Number of large read operations=0
		13/01/27 17:55:21 INFO mapred.JobClient:     HDFS: Number of write operations=10
		13/01/27 17:55:21 INFO mapred.JobClient:   Job Counters
		13/01/27 17:55:21 INFO mapred.JobClient:     Launched map tasks=10
		13/01/27 17:55:21 INFO mapred.JobClient:     Launched reduce tasks=10
		13/01/27 17:55:21 INFO mapred.JobClient:     Data-local map tasks=9
		13/01/27 17:55:21 INFO mapred.JobClient:     Rack-local map tasks=1
		13/01/27 17:55:21 INFO mapred.JobClient:     Total time spent by all maps in occupied slots (ms)=35368
		13/01/27 17:55:21 INFO mapred.JobClient:     Total time spent by all reduces in occupied slots (ms)=47336
		13/01/27 17:55:21 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
		13/01/27 17:55:21 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
		13/01/27 17:55:21 INFO mapred.JobClient:   Map-Reduce Framework
		13/01/27 17:55:21 INFO mapred.JobClient:     Map input records=3538
		13/01/27 17:55:21 INFO mapred.JobClient:     Map output records=67
		13/01/27 17:55:21 INFO mapred.JobClient:     Map output bytes=41172
		13/01/27 17:55:21 INFO mapred.JobClient:     Input split bytes=1480
		13/01/27 17:55:21 INFO mapred.JobClient:     Combine input records=0
		13/01/27 17:55:21 INFO mapred.JobClient:     Combine output records=0
		13/01/27 17:55:21 INFO mapred.JobClient:     Reduce input groups=3
		13/01/27 17:55:21 INFO mapred.JobClient:     Reduce shuffle bytes=42947
		13/01/27 17:55:21 INFO mapred.JobClient:     Reduce input records=67
		13/01/27 17:55:21 INFO mapred.JobClient:     Reduce output records=3
		13/01/27 17:55:21 INFO mapred.JobClient:     Spilled Records=134
		13/01/27 17:55:21 INFO mapred.JobClient:     CPU time spent (ms)=23900
		13/01/27 17:55:21 INFO mapred.JobClient:     Physical memory (bytes) snapshot=7343108096
		13/01/27 17:55:21 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=31596683264
		13/01/27 17:55:21 INFO mapred.JobClient:     Total committed heap usage (bytes)=15574695936
13/01/27 17:55:21 INFO driver.MahoutDriver: Program took 308644 ms (Minutes: 5.144066666666666)


2/ Clustering

